\documentclass[12pt]{article}

\usepackage{fullpage}
\usepackage{enumerate}

\title{
    \vfill
    Adaptive Difficulty in Connect Four \\
    COMP 4106
}
\author{
    Juhandr\'{e} Knoetze - 100882772
}

\begin{document}

\begin{titlepage}
    \maketitle
    \vfill
    \thispagestyle{empty}
\end{titlepage}
% X: Easy
% O: Adaptive
%Total Games: 150
%Player O Won: 22
%Player X Won: 28
%Player X Won: 34
%Player O Won: 16
%Player O Won: 14
%Player X Won: 36


% X: Hard
% O: Adaptive
%Total Games: 150
%Player O Won: 19
%Player X Won: 31
%Player O Won: 13
%Player X Won: 37
%Player O Won: 14
%Player X Won: 36



\section*{Problem Domain}


\section*{Motivation}
While having different difficulty options may allow the user to play against the AI agent at a better matching skill level, it does not guarantee the player will be a perfect match for the agent. It is possible the difficulty levels provided do not account for all skill levels of players. Players may find themselves unskilled or too skilled for any of the options provided, leaving them with a less enjoyable game.

Instead of the user deciding what difficulty the agent should play at, the agent adapts to the player's skill level. This will ensure that the agent is playing at a similar skill level as the player at all times. The agent will not be too difficult or too easy for the player giving them a more enjoyable game.

\section*{Implementation}
\subsection*{AI Techniques}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Adaptive Agent}
For the Adaptive Agent, AA, to be adaptive, its goal is not perform optimally. Instead, its objective is to perform approximately on par as its opponent. The AA must rank all of its possible moves at a given state of the game and select the move that is ranked similarly to that of the opponent's moves. To get the ranking of each of the AA's moves, following the work by Missura and Gaertner, a modified version of the Mini-Max algorithm is used.

Instead of performing Mini-Max on the current state of the game, the AA first determines all its possible moves. It applies each move separately to the state and performs Mini-Max on each new state. Performing Mini-Max on each new state will provide a heuristic value for the applied move. The heuristic value for the move determines the rank of the move for the state. The AA then selects a move from the set that is ranked similarly to the opponent's moves.

To evaluate the performance of the opponent, the same strategy is applied to the opponent's last move. Before the AA makes its move, it evaluates all the possible moves that could have been made by the opponent. It ranks all the moves the same as way as it does for its own move and adds it to a running average of the performance of the opponent.

The heuristic used for the AA is modified as well. If a player comes across a winning game state, the heuristic returns positive or negative infinity in the event the player wins or loses, respectively. As discussed by Missura and Gaertner, a winning state fewer moves away is more desirable, and therefore is ranked higher, than a winning state that requires more moves to reach. Similarly, a losing state more moves away is more desirable than losing state very close.


\end{document}